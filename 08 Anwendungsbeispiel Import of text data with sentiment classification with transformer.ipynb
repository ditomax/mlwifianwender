{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"header.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anwendungsbeispiel Import of text data with sentiment classification\n",
    "\n",
    "In diesem Beispiel werden wir Textdaten behandeln und versuchen die Stimmung eines kurzen Stückes Text zu bestimmen. Damit können zum Beispiel eMails oder Social Media Beiträge gefiltert werden. Diese Variane wurde mit Hilfe eines Transformers implementiert.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- [2] [https://stackabuse.com/python-for-nlp-movie-sentiment-analysis-using-deep-learning-in-keras/](https://stackabuse.com/python-for-nlp-movie-sentiment-analysis-using-deep-learning-in-keras/)\n",
    "- [3] https://gdcoder.com/sentiment-clas/\n",
    "- [4] [https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "\n",
    "\n",
    "Zitierungen GloVe [1] und Datensatz[2]:\n",
    "```\n",
    "[1] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation.\n",
    "\n",
    "[2] Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher, Learning Word Vectors for Sentiment Analysis, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, June 2011, Portland, Oregon, USA, Association for Computational Linguistics, http://www.aclweb.org/anthology/P11-1015\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import der Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-for-tf2 in /opt/homebrew/Caskroom/miniconda/base/envs/wificlass/lib/python3.10/site-packages (0.14.9)\n",
      "Requirement already satisfied: py-params>=0.9.6 in /opt/homebrew/Caskroom/miniconda/base/envs/wificlass/lib/python3.10/site-packages (from bert-for-tf2) (0.10.2)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/wificlass/lib/python3.10/site-packages (from bert-for-tf2) (0.8.2)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/wificlass/lib/python3.10/site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.65.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/wificlass/lib/python3.10/site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on keras version 2.11.0 on tensorflow 2.11.0 using hub 0.12.0\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Import der Module\n",
    "#\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "import string\n",
    "from urllib.request import urlretrieve\n",
    "import tarfile\n",
    "import zipfile\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Abdrehen von Fehlermeldungen\n",
    "#\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=Warning)\n",
    "simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "\n",
    "#\n",
    "# Tensorflow und Keras\n",
    "#\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Flatten, LSTM\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import  Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import bert\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Für GPU Support\n",
    "#\n",
    "tflogger = tf.get_logger()\n",
    "tflogger.setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR )\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "#\n",
    "# Einstellen der Grösse von Diagrammen\n",
    "#\n",
    "plt.rcParams['figure.figsize'] = [16, 9]\n",
    "\n",
    "\n",
    "#\n",
    "# Ausgabe der Versionen\n",
    "#\n",
    "print('working on keras version {} on tensorflow {} using hub {}'.format ( tf.keras.__version__, tf.version.VERSION, hub.__version__ ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konstanten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Konstanten für Dateien\n",
    "#\n",
    "urlDataSource = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "localExtractionFolder = 'data/moviereviews'\n",
    "localDataArchive = localExtractionFolder + '/aclImdb_v1.tar.gz'\n",
    "textData = localExtractionFolder + '/aclImdb/'\n",
    "size = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Laden der Daten von einer URL\n",
    "#\n",
    "def download_dataset(url,dataset_file_path,extraction_directory):\n",
    "    if (not os.path.exists(extraction_directory)):\n",
    "        os.makedirs(extraction_directory)\n",
    "    if os.path.exists(dataset_file_path):\n",
    "        print(\"archive already downloaded.\")\n",
    "    else:\n",
    "        print(\"started loading archive from url {}\".format(url))\n",
    "        filename, headers = urlretrieve(url, dataset_file_path)\n",
    "        print(\"finished loading archive from url {} to {}\".format(url,filename))\n",
    "\n",
    "def extract_dataset(dataset_file_path, extraction_directory):\n",
    "    if (not os.path.exists(extraction_directory)):\n",
    "        os.makedirs(extraction_directory)\n",
    "    if (dataset_file_path.endswith(\"tar.gz\") or dataset_file_path.endswith(\".tgz\")):\n",
    "        tar = tarfile.open(dataset_file_path, \"r:gz\")\n",
    "        tar.extractall(path=extraction_directory)\n",
    "        tar.close()\n",
    "    elif (dataset_file_path.endswith(\"tar\")):\n",
    "        tar = tarfile.open(dataset_file_path, \"r:\")\n",
    "        tar.extractall(path=extraction_directory)\n",
    "        tar.close()\n",
    "    print(\"extraction of dataset from {} to {} done.\".format(dataset_file_path,extraction_directory) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laden und erster Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive already downloaded.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Laden der Daten ausführen\n",
    "#\n",
    "download_dataset(urlDataSource,localDataArchive,localExtractionFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extraction of dataset from data/moviereviews/aclImdb_v1.tar.gz to data/moviereviews done.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Extrahieren der Daten\n",
    "#\n",
    "extract_dataset(localDataArchive,localExtractionFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wie sehen die Daten auf dem Filesystem aus?\n",
    "\n",
    "<img src=\"info.png\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Sammeln der Daten aus den Files\n",
    "#\n",
    "def load_texts_labels_from_folders(path, folders):\n",
    "    print('scanning path {}'.format(path))\n",
    "    texts,labels = [],[]\n",
    "    for idx,label in enumerate(folders):\n",
    "        print('scanning {}'.format(idx))\n",
    "        count = 0\n",
    "        for fname in glob(os.path.join(path, label, '*.*')):\n",
    "            if count > size:\n",
    "                break\n",
    "            texts.append(open(fname, 'r').read())\n",
    "            labels.append(idx)\n",
    "            count += 1\n",
    "    return texts, np.array(labels).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning path data/moviereviews/aclImdb/train\n",
      "scanning 0\n",
      "scanning 1\n",
      "scanning path data/moviereviews/aclImdb/test\n",
      "scanning 0\n",
      "scanning 1\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Laden der positiven und negativen Beispiele\n",
    "#\n",
    "classes = ['neg','pos']\n",
    "x_train,y_train = load_texts_labels_from_folders( textData + 'train', classes)\n",
    "x_test,y_test = load_texts_labels_from_folders( textData + 'test', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4002, 4002, 4002, 4002)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train),len(y_train),len(x_test),len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, numpy.ndarray)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Prüfen des Datentypen\n",
    "#\n",
    "(type(x_train),type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Prüfen der Klassen\n",
    "#\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\n",
      "label 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# negative Beispiele\n",
    "#\n",
    "for index in range (0,1):\n",
    "    print(x_train[index])\n",
    "    print(\"label {}\".format(y_train[index]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we have here is a damn good little nineties thriller that, while perhaps lacking in substance, still provides great entertainment throughout it's running time and overall does everything you could possibly want a film of this nature to do. I saw this film principally because it was directed by John Dahl - a highly underrated director behind great thrillers such as The Last Seduction, Rounders and Roadkill. I figured that if this film was up to standard of what I've already seen from the director, it would be well worth watching - and Red Rock West is certainly a film that Dahl can be proud of. The plot focuses on the overly moral Michael; a man travelling across America looking for work. He ends up finding it one day when he stumbles upon a bar in Red Rock County - only catch is that the job is to murder a man's wife. He's been mistaken for a killer named Lyle, but instead of doing the job; he plays both sides against each other and eventually plans to make a getaway. However, his attempts to escape are unsuccessful and he finds himself in a bad situation when the real Lyle turns up...<br /><br />John Dahl appears to enjoy setting thrillers on the road; he did it three years earlier with Kill Me Again, and again almost a decade on from this film with Roadkill. It's not hard to see why Dahl chooses this sort of location, as it provides a fabulous atmosphere for a thriller the likes of this one. Dahl also provides his film with a 'film noir' like atmosphere, as the plot mainly focuses on the central character and the word he is plunged into is full of dark and mysterious characters. The acting is largely very good, with Nicholas Cage doing an excellent job in the lead role, and getting A-class support from Lara Flynn Boyle, J.T. Walsh and, of course, Dennis Hopper; who once again commands the screen with his over the top performance. It has to be said that the second half of the film isn't as gripping as the first, but Red Rock West certainly is never boring and the way that Dahl orchestrates the grand finale is excellent in that all the central characters get to be a part of it. Overall, Red Rock West is a film that you're unlikely to regret watching. It's thrilling throughout, and you can't ask for much more than that!\n",
      "label 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# positive Beispiele\n",
    "#\n",
    "for index in range (13001,13002):\n",
    "    print(x_train[index])\n",
    "    print(\"label {}\".format(y_train[index]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for constructing BERT Embeddings: input_ids, input_masks, input_segments and Inputs\n",
    "MAX_SEQ_LEN=500 # max sequence length\n",
    "\n",
    "def get_masks(tokens):\n",
    "    \"\"\"Masks: 1 for real tokens and 0 for paddings\"\"\"\n",
    "    return [1]*len(tokens) + [0] * (MAX_SEQ_LEN - len(tokens))\n",
    " \n",
    "def get_segments(tokens):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"  \n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (MAX_SEQ_LEN - len(tokens))\n",
    "\n",
    "def get_ids(tokens, tokenizer):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
    "    input_ids = token_ids + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "def create_single_input(sentence, tokenizer, max_len):\n",
    "    \"\"\"Create an input from a sentence\"\"\"\n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "    stokens = stokens[:max_len]\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    " \n",
    "    ids = get_ids(stokens, tokenizer)\n",
    "    masks = get_masks(stokens)\n",
    "    segments = get_segments(stokens)\n",
    "\n",
    "    return ids, masks, segments\n",
    " \n",
    "def convert_sentences_to_features(sentences, tokenizer):\n",
    "    \"\"\"Convert sentences to features: input_ids, input_masks and input_segments\"\"\"\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    " \n",
    "    for sentence in sentences:\n",
    "      ids,masks,segments=create_single_input(sentence,tokenizer,MAX_SEQ_LEN-2)\n",
    "      assert len(ids) == MAX_SEQ_LEN\n",
    "      assert len(masks) == MAX_SEQ_LEN\n",
    "      assert len(segments) == MAX_SEQ_LEN\n",
    "      input_ids.append(ids)\n",
    "      input_masks.append(masks)\n",
    "      input_segments.append(segments)\n",
    "\n",
    "    return [np.asarray(input_ids, dtype=np.int32), \n",
    "          np.asarray(input_masks, dtype=np.int32), \n",
    "          np.asarray(input_segments, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tonkenizer(bert_layer):\n",
    "    \"\"\"Instantiate Tokenizer with vocab\"\"\"\n",
    "    vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    do_lower_case=bert_layer.resolved_object.do_lower_case.numpy() \n",
    "    tokenizer=bert.bert_tokenization.FullTokenizer(vocab_file,do_lower_case)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_model(callable_object):\n",
    "    # Load the pre-trained BERT base model\n",
    "    bert_layer = hub.KerasLayer(handle=callable_object, trainable=True)  \n",
    "   \n",
    "    # BERT layer three inputs: ids, masks and segments\n",
    "    input_ids = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_ids\")           \n",
    "    input_masks = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_masks\")       \n",
    "    input_segments = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"segment_ids\")\n",
    "    \n",
    "    inputs = [input_ids, input_masks, input_segments] # BERT inputs\n",
    "    pooled_output, sequence_output = bert_layer(inputs) # BERT outputs\n",
    "    \n",
    "    # Add a hidden layer\n",
    "    x = Dense(units=768, activation='relu')(pooled_output)\n",
    "    x = Dropout(0.1)(x)\n",
    " \n",
    "    # Add output layer\n",
    "    outputs = Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    # Construct a new model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " input_masks (InputLayer)       [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " keras_layer_1 (KerasLayer)     [(None, 768),        109482241   ['input_ids[0][0]',              \n",
      "                                 (None, 500, 768)]                'input_masks[0][0]',            \n",
      "                                                                  'segment_ids[0][0]']            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 768)          590592      ['keras_layer_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 768)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            1538        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110,074,371\n",
      "Trainable params: 110,074,370\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = nlp_model(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tonkenizer(model.layers[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = convert_sentences_to_features(x_train, tokenizer)\n",
    "X_test = convert_sentences_to_features(x_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_categorical(y_train)\n",
    "Y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1\n",
    "\n",
    "# Use Adam optimizer to minimize the categorical_crossentropy loss\n",
    "opt = Adam(learning_rate=2e-5)\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251/251 [==============================] - 15064s 60s/step - loss: 0.3523 - accuracy: 0.8393 - val_loss: 0.2225 - val_accuracy: 0.9145\n"
     ]
    }
   ],
   "source": [
    "# Fit the data to the model\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_name = 'results/bert_model.h5'\n",
    "model.save(model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained nlp_model\n",
    "new_model = load_model(model_name,custom_objects={'KerasLayer':hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 2614s 21s/step\n"
     ]
    }
   ],
   "source": [
    "yhat_test = new_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91      2001\n",
      "           1       0.90      0.93      0.92      2001\n",
      "\n",
      "    accuracy                           0.91      4002\n",
      "   macro avg       0.91      0.91      0.91      4002\n",
      "weighted avg       0.91      0.91      0.91      4002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.argmax(Y_test,axis=1), np.argmax(yhat_test,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test mit neuen Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    \n",
    "    sentences = [text]\n",
    "    \n",
    "    features = convert_sentences_to_features( sentences, tokenizer)\n",
    "    sentiment = model.predict(features)[0][1]\n",
    "    \n",
    "    comment = 'meh'\n",
    "    if sentiment > 0.85:\n",
    "        comment = 'very good'\n",
    "    elif sentiment > 0.75:\n",
    "        comment = 'good'\n",
    "    elif sentiment > 0.50:\n",
    "        comment = 'moderate'\n",
    "    return sentiment,comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 373ms/step\n",
      "(0.016892947, 'meh')\n"
     ]
    }
   ],
   "source": [
    "test1 = \"I simply don't like this film. I does not reach to any expectations.\"\n",
    "print ( sentiment(test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 370ms/step\n",
      "(0.015629997, 'meh')\n"
     ]
    }
   ],
   "source": [
    "test1 = \"I hate this film. It is the worst garbage on earth.\"\n",
    "print ( sentiment(test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 371ms/step\n",
      "(0.99177223, 'very good')\n"
     ]
    }
   ],
   "source": [
    "test1 = \"This is one of the best movies on earth.\"\n",
    "print ( sentiment(test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weiterführende Schritte\n",
    "\n",
    "\n",
    "Stimmungsanalyse für Deutsch [https://machine-learning-blog.de/2019/06/03/stimmungsanalyse-sentiment-analysis-auf-deutsch-mit-python/](https://machine-learning-blog.de/2019/06/03/stimmungsanalyse-sentiment-analysis-auf-deutsch-mit-python/)\n",
    "\n",
    "Anleitung für Zugriff auf twitter API [https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/](https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/)\n",
    "\n",
    "<img src=\"info.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
