{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"header.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anwendungsbeispiel Incremental/active learning and optimization\n",
    "\n",
    "Das Ziel dieses Beispiels ist die Erklärung von **incremental learning** und **active learning**. Dabei wird versucht aus neuen Daten weitere Trainingsdaten zu generieren mit denen ein Modell verbessert werden kann. Im zweiten Teil wird noch die Technik der **hyperparameter optimization** vorgestellt.\n",
    "\n",
    "\n",
    "Der Code für die hyperparameter optimization wurde aus [1] entlehnt.\n",
    "\n",
    "- [1] [https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce](https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce)\n",
    "\n",
    "\n",
    "Referenz auf den EMNIST Datensatz:\n",
    "```\n",
    "EMNIST: Extending MNIST to handwritten letters, Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Schaik, Andre Van, 2017 International Joint Conference on Neural Networks (IJCNN), 2017\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import der Module  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Einstellen der Grösse von Diagrammen\n",
    "#\n",
    "plt.rcParams['figure.figsize'] = [16, 14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Learning\n",
    "\n",
    "Bei incremental learning werden aus neuen Daten mit Hilfe von Regeln neue Trainingsdaten extrahiert und damit ein Modell neu trainiert. Wir werden sehen, ob das funktioniert.\n",
    "\n",
    "Mehr zu incremental learning unter [https://en.wikipedia.org/wiki/Incremental_learning](https://en.wikipedia.org/wiki/Incremental_learning)\n",
    "\n",
    "<img src=\"info.png\" align=\"left\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das alte Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Parameter für Modell\n",
    "#\n",
    "prefix = 'results/04_'\n",
    "modelName = prefix + \"model.json\"\n",
    "weightName = prefix + \"model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Laden des vortrainierten Modelles aus Anwendungsbeispiel 04\n",
    "#\n",
    "json_file = open(modelName, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(weightName)\n",
    "print(\"loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Vorbereitung für Test\n",
    "#\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "loaded_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# MNIST Daten (wurden für das Trainings des alten Modelles verwendet)\n",
    "#\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Messen der Accuracy des alten Modelles\n",
    "#\n",
    "_, acc = loaded_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Accuracy {:.5f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Speichern für später\n",
    "#\n",
    "oldModelAccuracy = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neue Daten aus dem echten Betrieb\n",
    "\n",
    "Wir simulieren neue Daten indem wir Daten aus einem anderen Datensatz EMNIST verwenden. Im Produktiveinsatz treten in der Regel immer neue Daten auf. Das Model wurde auf MNIST trainiert und muss jetzt die EMNIST Daten klassifieren. Diese Daten hat das Modell noch nie gesehen. Es sind also Überraschungen zu erwarten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Laden der neuen EMNIST Daten\n",
    "#\n",
    "# y_new = np.loadtxt('data/emnist_labels.csv',dtype=np.int8,delimiter=\",\")\n",
    "#\n",
    "x_new = np.loadtxt('data/emnist_images.csv',delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.reshape(x_new,(-1,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Anzeige von Beispielen der Daten\n",
    "#\n",
    "for i in range(16):\n",
    "    plt.subplot(4,4,1 + i)\n",
    "    image = x_new[i].reshape((28,28))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('Greys_r'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Aus einem unklaren Grund sind die EMNIST Daten gedreht. Das muss korrigiert werden.\n",
    "#\n",
    "x_new_swap = np.swapaxes(x_new, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Anzeige von Beispielen der Daten\n",
    "#\n",
    "for i in range(16):\n",
    "    plt.subplot(4,4,1 + i)\n",
    "    image = x_new_swap[i].reshape((28,28))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('Greys_r'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Transformation\n",
    "#\n",
    "x_new_swap = x_new_swap.astype('float32')\n",
    "x_new_swap = x_new_swap / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Suche nach neuen Trainingsdaten\n",
    "#\n",
    "# Die Idee dahinter ist, dass wir nach Klassifizierungen suchen, die in einem bestimmten Bereich der confidence\n",
    "# liegen. Zu hohe confidence steigert das Risiko, dass wir eh schon bekannte Daten dazunehmen, zu niedrige confidence\n",
    "# steigert das Risiko, dass wir falsche Klassifizierungen dazunehmen.\n",
    "#\n",
    "\n",
    "candidateImages = []\n",
    "candidateLabels = []\n",
    "candidateConfidence = []\n",
    "candidateIndex = []\n",
    "candidateCount = 0\n",
    "classDistribution = [0] * 10\n",
    "\n",
    "for i in range(x_new_swap.shape[0]):\n",
    "    \n",
    "    image = x_new_swap[i].reshape((1,28,28,1))\n",
    "\n",
    "    prediction_activation = loaded_model.predict([image])\n",
    "    predictedClass = np.argmax ( prediction_activation[0] )\n",
    "    confidence = prediction_activation[0][predictedClass]\n",
    "    \n",
    "    if confidence > 0.7 and confidence < 0.9:\n",
    "        \n",
    "        classDistribution[predictedClass]+= 1\n",
    "        candidateCount+= 1\n",
    "        candidateIndex.append(i)\n",
    "        candidateImages.append(image)\n",
    "        candidateLabels.append(predictedClass)\n",
    "        candidateConfidence.append(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Anzahl der gefundenen Kandidaten ist {}'.format(candidateCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Anzeige von Beispielen der Kandidaten\n",
    "#\n",
    "for i in range(16):\n",
    "    ax = plt.subplot(4,4,1 + i) \n",
    "    ax.set_title('{} pred {} conf {:.2f}'.format ( candidateIndex[i], str(candidateLabels[i]), candidateConfidence[i] ))\n",
    "    image = candidateImages[i].reshape((28,28))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Manuelle Auswahl der Kandidaten durch den Domain Experten (active learning)\n",
    "#\n",
    "candidatesUsed =       [ 77,110,130,142,210,240,310,363,365,454,445,412,367,212]\n",
    "candidatesUsedLabels = [  2,  9,  9,  9,  2,  3,  7,  3,  9,  1,  9,  9,  8,  3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Sammeln der Daten aus der Liste\n",
    "#\n",
    "candidateImagesUsed = []\n",
    "candidateLabelsUsed = []\n",
    "for candidate in range(len(candidatesUsed)):\n",
    "    candidateLabelsUsed.append(candidatesUsedLabels[candidate])\n",
    "    candidateImagesUsed.append(candidateImages[candidate])\n",
    "    \n",
    "x_train_used = np.asarray ( candidateImagesUsed )\n",
    "y_train_used = np.asarray ( candidateLabelsUsed )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Prepare new data for training\n",
    "#\n",
    "x_train_used = x_train_used.reshape((x_train_used.shape[0], 28, 28, 1))\n",
    "y_train_used = to_categorical(y_train_used, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Zusammenführen der neuen und der alten Daten\n",
    "#\n",
    "x_train_active = np.concatenate((x_train_used, x_train), axis=0)\n",
    "y_train_active = np.concatenate((y_train_used, y_train), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_active.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_active.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Prepare Model for retraining\n",
    "#\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "loaded_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Training mit neuen Daten\n",
    "#\n",
    "history = loaded_model.fit(x_train_active, y_train_active, validation_data=(x_test,y_test), batch_size=64, epochs=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Messen der Accuracy des alten Modelles\n",
    "#\n",
    "_, acc = loaded_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('New model accuracy {:.5f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Alte model accuracy zum Vergleich \n",
    "#\n",
    "print('Old model accuracy {:.5f}'.format(oldModelAccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "\n",
    "\n",
    "Mehr zu hyperparameter optimization ist hier zu finden: \n",
    "- [https://towardsdatascience.com/hyperparameters-optimization-526348bb8e2d](https://towardsdatascience.com/hyperparameters-optimization-526348bb8e2d)\n",
    "- [https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)\n",
    "\n",
    "\n",
    "<img src=\"info.png\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Vorbereitung der MNIST Daten für RFC\n",
    "#\n",
    "x_train_rfc = np.reshape(x_train,(-1,28*28))\n",
    "x_train_rfc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rfc = np.argmax(y_train,axis=1)\n",
    "y_train_rfc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_rfc = np.reshape(x_test,(-1,28*28))\n",
    "y_test_rfc = np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators=20,max_depth=6,max_features=0.2,n_jobs=-1)\n",
    "clf_rf.fit(x_train_rfc, y_train_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = clf_rf.predict(x_test_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rf = accuracy_score(y_test_rfc, y_pred_rf)\n",
    "print (\"random forest accuracy: \",acc_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wie finden wir die optimalen Parameter?\n",
    "\n",
    "Es gibt mehrere python Module für die automatische Parameter Optimierung. Eines davon ist hyperopt. Hintergrundinformation dazu ist zu finden unter: [https://conference.scipy.org/proceedings/scipy2013/pdfs/bergstra_hyperopt.pdf](https://conference.scipy.org/proceedings/scipy2013/pdfs/bergstra_hyperopt.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import os\n",
    "\n",
    "# um mehr als einen worker zu verwenden sollte JOBLIB_TEMP_FOLDER global gesetzt sein\n",
    "os.environ[\"JOBLIB_TEMP_FOLDER\"] = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_train_test(params):\n",
    "    clf = RandomForestClassifier( n_jobs=-1, **params)\n",
    "    return cross_val_score(clf, x_train_rfc, y_train_rfc).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space4rf = {\n",
    "    'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "    'max_features': hp.uniform('max_features', 0, 1 ),\n",
    "    'n_estimators': hp.choice('n_estimators', range(1,30)),\n",
    "    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = 0\n",
    "def f(params):\n",
    "    global best\n",
    "    acc = hyperopt_train_test (params)\n",
    "    if acc > best:\n",
    "        best = acc\n",
    "        print ( 'new best accuracy {}:{}'.format( best, params ) )\n",
    "    return {'loss': -acc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best = fmin(f, space4rf, algo=tpe.suggest, max_evals=20, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('found optimum {}'.format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = ['n_estimators', 'max_depth', 'max_features', 'criterion', 'scale', 'normalize']\n",
    "f, axes = plt.subplots(nrows=2, ncols=3, figsize=(15,10))\n",
    "cmap = plt.cm.jet\n",
    "for i, val in enumerate(parameters):\n",
    "    print i, val\n",
    "    xs = np.array([t['misc']['vals'][val] for t in trials.trials]).ravel()\n",
    "    ys = [-t['result']['loss'] for t in trials.trials]\n",
    "    xs, ys = zip(\\*sorted(zip(xs, ys)))\n",
    "    ys = np.array(ys)\n",
    "    axes[i/3,i%3].scatter(xs, ys, s=20, linewidth=0.01, alpha=0.5, c=cmap(float(i)/len(parameters)))\n",
    "    axes[i/3,i%3].set_title(val)\n",
    "    #axes[i/3,i%3].set_ylim([0.9,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "# https://medium.com/@sebastiannorena/some-model-tuning-methods-bfef3e6544f0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
