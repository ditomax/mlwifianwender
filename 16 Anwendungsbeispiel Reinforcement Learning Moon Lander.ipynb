{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"header_profi.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Reinforcement Learning Moon Lander\n",
    "\n",
    "\n",
    "The goal of this exercise is to work with reinforcement learning models and get a basic understanding of the topic. We will first develop controlers for the simple cart pole model and later for the lunar lander.\n",
    "Neil Armstrong was the first to control a lunar lander in 1969. See a [video](https://youtu.be/xc1SzgGhMKc?t=520) about this masterpiece.\n",
    "Luckily we do not have to go to the moon, but can do our experiments in simulation based on the [Openai gym](https://gym.openai.com/) software.\n",
    "\n",
    "\n",
    "**Note**: openai gym is not well supported in anaconda. Please install gym in your conda environment using the following command:\n",
    "\n",
    "```\n",
    "pip install gym\n",
    "pip install box2d box2d-kengz\n",
    "```\n",
    "\n",
    "**Note**: it can happend that the rendering window does not show up or close properly. In this case please check your environment and look for a solution and post it in the forum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress some warnings\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ( tf.__version__ ) \n",
    "print(gym.__version__)\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very basic RL example\n",
    "\n",
    "Run this basic cart pole example and find out how it works and what the basic functions of gym are. Document the code with python comments. Find out what the observation and action values mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "cumulated_reward = 0\n",
    "for i in range(200):\n",
    "\n",
    "    env.render()\n",
    "    \n",
    "    # pick random action from action space\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # push action to environment and get back state, reward, done flag and more information\n",
    "    observation, reward, done, info = env.step( action )\n",
    "    \n",
    "    # sum reward\n",
    "    cumulated_reward += reward\n",
    "    \n",
    "    print( '\\r', 'round: {} angle:{:.2f} position:{:.2f} rew:{} cumrew:{} done:{}   action:{}     '.format(i,observation[2],observation[0],reward,cumulated_reward,done,action), end='' )\n",
    "    \n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "    # some delay important for display to catch up\n",
    "    time.sleep(0.1)\n",
    "      \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a basic on-off control strategy\n",
    "\n",
    "Before we go into advanced control strategies, lets attempt to control the cart pole with a simple on-off control strategy. Reading the [documentation](https://github.com/openai/gym/wiki/CartPole-v0) of this gym we find that it has two actions (push cart left = 0 and push cart right = 1). So, one idea could be to just look at the pole's angle and push the cart left if the pole leans to the left and vice versa. Give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "\n",
    "cumulated_reward = 0\n",
    "pole_angle = 0\n",
    "\n",
    "for i in range(200):\n",
    "\n",
    "    env.render()\n",
    "    \n",
    "    # Result: implement your control strategy here\n",
    "    if pole_angle < 0:\n",
    "        action = 1\n",
    "    else:\n",
    "        action = 0\n",
    "\n",
    "    observation, reward, done, info = env.step( action )\n",
    "    cumulated_reward += reward\n",
    "    pole_angle = observation[0]\n",
    "\n",
    "    print( '\\r', 'round: {} angle:{:.2f} position:{:.2f} rew:{} cumrew:{} done:{}   action:{}     '.format(i,observation[2],observation[0],reward,cumulated_reward,done,action), end='' )\n",
    "    \n",
    "    if done:\n",
    "        env.reset()\n",
    "        cululated_reward = 0\n",
    "\n",
    "    # some delay important for display to catch up\n",
    "    time.sleep(0.1)\n",
    "      \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Solution to cart pole balancing\n",
    "\n",
    "Now lets build a first version based on advanced RL technique, the Deep Q-Network. Here a neural network is trained to estimate the best action for a state based on the Q-learning concept.\n",
    "\n",
    "The code is based on the work by Greg Surma and it can be found [here](https://github.com/gsurma/cartpole).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "prefix = 'results/16_dqn_'\n",
    "\n",
    "# hyperparameters from https://towardsdatascience.com/ai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055\n",
    "\n",
    "GAMMA = 0.98\n",
    "LEARNING_RATE = 0.002\n",
    "LEARNING_RATE_DECAY = 0.9990\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 16\n",
    "EXPLORATION_MAX = 0.75\n",
    "EXPLORATION_MIN = 0.1\n",
    "EXPLORATION_DECAY = 0.98\n",
    "layout=[10,10]\n",
    "\n",
    "class DQNControl:\n",
    "\n",
    "    def __init__(self, observation_space, action_space,layout=[24,24],name='nona'):\n",
    "        \n",
    "        print ('building DQN model with observation space {} and action space {} layer {} name {}'.format(observation_space, action_space,layout,name) )\n",
    "        \n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.name = name\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(layout[0], input_shape=(observation_space,), activation=\"relu\"))\n",
    "        self.model.add(Dense(layout[1], activation=\"relu\"))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        \n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                        initial_learning_rate=LEARNING_RATE,\n",
    "                        decay_steps=1000,\n",
    "                        decay_rate=LEARNING_RATE_DECAY)        \n",
    "        \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(learning_rate=lr_schedule ))\n",
    "\n",
    "        \n",
    "    def save(self):\n",
    "        modelName = prefix + self.name + \"model.json\"\n",
    "        weightName = prefix + self.name + \"model.h5\"\n",
    "        model_json = self.model.to_json()\n",
    "        with open( modelName , \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        self.model.save_weights( weightName )\n",
    "        print(\"saved model to disk as {} {}\".format(modelName,weightName))\n",
    "\n",
    "        \n",
    "    def load(self):    \n",
    "        modelName = prefix + self.name + \"model.json\"\n",
    "        weightName = prefix + self.name + \"model.h5\"\n",
    "        json_file = open(modelName, 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        self.model = model_from_json(loaded_model_json)\n",
    "        self.model.load_weights(weightName)\n",
    "        print(\"loaded model from disk\")\n",
    "        \n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        \n",
    "    def action(self,state):\n",
    "        q_values = self.model.predict(state,verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def act(self, state):\n",
    "        #\n",
    "        # Task: what is the purpose of this if statement\n",
    "        # Result: ....\n",
    "        #\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "\n",
    "        q_values = self.model.predict(state,verbose=0)\n",
    "        \n",
    "        #\n",
    "        # Task: what is the idea behind this step (to come from value to action)?\n",
    "        # Result: ....\n",
    "        #\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        \n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        \n",
    "        for state, action, reward, state_next, done in batch:\n",
    "            \n",
    "            q_update = reward\n",
    "            if not done:\n",
    "                #\n",
    "                # Task: give an explanation for the formula of the update of the Q-value\n",
    "                #\n",
    "                q_update = (reward + GAMMA * np.amax( self.model.predict(state_next, verbose=0)[0] ) )\n",
    "            \n",
    "            q_values = self.model.predict(state,verbose=0)\n",
    "            \n",
    "            q_values[0][action] = q_update\n",
    "            \n",
    "            self.model.fit(state, q_values, verbose=0)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def close_episode(self):\n",
    "        #\n",
    "        # Task: what is going on here?\n",
    "        # Result: ...\n",
    "        #\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDQN(env,episodes=50,layout=[24,24], name='nona', termination_reward=None, termination_runs=None, termination_runs_reward=None ):\n",
    "    \n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "\n",
    "    dqn_solver = DQNControl(observation_space, action_space,layout,name)\n",
    "    \n",
    "    history = []\n",
    "    run = 0\n",
    "    \n",
    "    accumulated_reward = 0\n",
    "    sliding_accumulated_reward = 0\n",
    "    \n",
    "    while run < episodes:\n",
    "        \n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        step = 0\n",
    "        while True:\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            #env.render()\n",
    "            \n",
    "            action = dqn_solver.act(state)\n",
    "            \n",
    "            state_next, reward, terminal, info = env.step(action)\n",
    "            \n",
    "            accumulated_reward += reward\n",
    "            \n",
    "            if not (termination_runs is None) and step > termination_runs:\n",
    "                terminal = True\n",
    "                if not (termination_runs_reward is None):\n",
    "                    reward = termination_runs_reward\n",
    "            else:\n",
    "                if terminal and not (termination_reward is None):\n",
    "                    reward = termination_reward\n",
    "            \n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)\n",
    "            \n",
    "            state = state_next\n",
    "            \n",
    "            if terminal:\n",
    "                \n",
    "                sliding_accumulated_reward = sliding_accumulated_reward * 0.9 + accumulated_reward * 0.1\n",
    "                \n",
    "                print ( '\\r', 'episode: {}, exploration: {:.3f}, score: {} sliding score {}'.format(run,dqn_solver.exploration_rate,accumulated_reward,sliding_accumulated_reward), end='' )\n",
    "                \n",
    "                history.append([run,dqn_solver.exploration_rate,accumulated_reward,sliding_accumulated_reward,step])\n",
    "                \n",
    "                accumulated_reward = 0\n",
    "                break\n",
    "            \n",
    "            dqn_solver.experience_replay()\n",
    "        \n",
    "        \n",
    "        dqn_solver.close_episode()\n",
    "        \n",
    "        \n",
    "        run += 1\n",
    "\n",
    "    return dqn_solver,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control,history = trainDQN(env=env,episodes=100,layout=layout,name='cartdqn',termination_reward=-200,termination_runs=100,termination_runs_reward=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for later\n",
    "control.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[2].plot()\n",
    "df[3].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the DQL control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "env.reset()\n",
    "\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "control = DQNControl(observation_space, action_space, name='cartdqn',layout=layout)\n",
    "control.load()\n",
    "\n",
    "state = env.reset()\n",
    "cumulated_reward = 0\n",
    "\n",
    "for i in range(100):\n",
    "    env.render(mode='close')\n",
    "\n",
    "    # Result: implement your control strategy here\n",
    "    action = control.action( np.reshape(state, [1, observation_space]) )\n",
    "    observation, reward, done, _ = env.step( action )\n",
    "    \n",
    "    cumulated_reward += reward\n",
    "        \n",
    "    print( '\\r', 'round: {} angle:{:.2f} position:{:.2f} rew:{} cumrew:{} done:{}   action:{}     '.format(i,observation[2],observation[0],reward,cumulated_reward,done,action), end='' )\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        cumulated_reward = 0\n",
    "\n",
    "    # some delay important for display to catch up\n",
    "    time.sleep(0.05)\n",
    "      \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PPO for Cart Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel environments\n",
    "vec_env = make_vec_env(\"CartPole-v1\", n_envs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", vec_env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model # remove to demonstrate saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = vec_env.reset()\n",
    "for episode in range(200):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar lander problem\n",
    "\n",
    "How we are looking into the lunar lander problem. We reuse the DQN controller from above with different parameters. Play with this problem and get an understanding of the rewards. Configuration is taken from [2]. A general discussion about this approach was published in [1].\n",
    "\n",
    "- [1] https://www.researchgate.net/publication/333145451_Deep_Q-Learning_on_Lunar_Lander_Game\n",
    "- [2] https://towardsdatascience.com/ai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(\"LunarLander-v2\", n_envs=4)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 100000\n",
    "ITERATIONS = 10\n",
    "for iteration in range(ITERATIONS):\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False,log_interval=10)\n",
    "    model.save(f\"{models_dir}/lunar_ppo_{iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"{models_dir}/lunar_ppo_9.zip\"\n",
    "model = PPO.load(model_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for ep in range(episodes):\n",
    "    obs = env.reset()\n",
    "\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    \n",
    "    while not done.any():\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        env.render()\n",
    "        print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
