{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"header_anwender.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anwendungsbeispiel Import of image data with augmentation and classification\n",
    "\n",
    "Das Ziel dieses Beispieles ist es die Organisation, den Import und die Vorbereitung von Bilddaten für eine Klassifikation zu erklären. Dabei werden folgende Schritte durchgeführt:\n",
    "\n",
    "- Dynamisches Laden und entpacken der Bilddaten von einer externen Quelle\n",
    "- Review der Organisation auf dem Filesystem\n",
    "- Laden der Daten\n",
    "- Transformationen\n",
    "- Augmentierung\n",
    "- Training\n",
    "- Analyse\n",
    "- Verbesserung\n",
    "\n",
    "Der verwendete Datensatz heisst caltech101[3] mit 101 Klassen und jeweils 40 bis 800 Bildern pro Klasse. Die Bilder haben 200 - 300 Pixel Auflösung in Farbe.\n",
    "\n",
    "Quellen für die Beispiele und Daten:\n",
    "\n",
    "- [1] [https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/](https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/)\n",
    "- [2] [https://github.com/bhavul/Caltech-101-Object-Classification](https://github.com/bhavul/Caltech-101-Object-Classification)\n",
    "- [3] [http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Import der Module\n",
    "#\n",
    "import os\n",
    "import logging\n",
    "import tarfile\n",
    "import operator\n",
    "import random\n",
    "from urllib.request import urlretrieve\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "#\n",
    "# Abdrehen von Fehlermeldungen\n",
    "#\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=Warning)\n",
    "simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "\n",
    "#\n",
    "# Tensorflow und Keras\n",
    "#\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, Input, Dropout, Activation, Dense, MaxPooling2D, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#\n",
    "# Für GPU Support\n",
    "#\n",
    "tflogger = tf.get_logger()\n",
    "tflogger.setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR )\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "#\n",
    "# Einstellen der Grösse von Diagrammen\n",
    "#\n",
    "plt.rcParams['figure.figsize'] = [16, 9]\n",
    "\n",
    "\n",
    "#\n",
    "# Ausgabe der Versionen\n",
    "#\n",
    "print('working on keras version {} on tensorflow {} using sklearn {}'.format ( tf.keras.__version__, tf.version.VERSION, sklearn.__version__ ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hilfsfunktionen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlDataSource = 'http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz'\n",
    "localExtractionFolder = 'data/caltech101'\n",
    "localDataArchive = 'data/caltech101/caltech101.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Laden der Daten von einer URL\n",
    "#\n",
    "def download_dataset(url,dataset_file_path):\n",
    "    if os.path.exists(localDataArchive):\n",
    "        print(\"archive already downloaded.\")\n",
    "    else:\n",
    "        print(\"started loading archive from url {}\".format(url))\n",
    "        filename, headers = urlretrieve(url, dataset_file_path)\n",
    "        print(\"finished loading archive from url {}\".format(url))\n",
    "\n",
    "def extract_dataset(dataset_file_path, extraction_directory):\n",
    "    if (not os.path.exists(extraction_directory)):\n",
    "        os.makedirs(extraction_directory)\n",
    "    if (dataset_file_path.endswith(\"tar.gz\") or dataset_file_path.endswith(\".tgz\")):\n",
    "        tar = tarfile.open(dataset_file_path, \"r:gz\")\n",
    "        tar.extractall(path=extraction_directory)\n",
    "        tar.close()\n",
    "    elif (dataset_file_path.endswith(\"tar\")):\n",
    "        tar = tarfile.open(dataset_file_path, \"r:\")\n",
    "        tar.extractall(path=extraction_directory)\n",
    "        tar.close()\n",
    "    print(\"extraction of dataset from {} to {} done.\".format(dataset_file_path,extraction_directory) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laden der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Laden der Daten ausführen\n",
    "#\n",
    "download_dataset(urlDataSource,localDataArchive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Extrahieren der Daten\n",
    "#\n",
    "extract_dataset(localDataArchive,localExtractionFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organisation von Bilddaten auf dem Filesystem\n",
    "\n",
    "Eine gute Einführung in das Thema ist zu finden unter\n",
    "\n",
    "- [Brownlee](https://machinelearningmastery.com/how-to-load-large-datasets-from-directories-for-deep-learning-with-keras/) \n",
    "- [Sarkar](https://towardsdatascience.com/a-single-function-to-streamline-image-classification-with-keras-bd04f5cfe6df)\n",
    "\n",
    "<img src=\"info.png\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erzeugen der Trainingsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Hilfsfunktionen\n",
    "#\n",
    "\n",
    "def get_images(object_category, data_directory):\n",
    "    if (not os.path.exists(data_directory)):\n",
    "        print(\"data directory not found.\")\n",
    "        return\n",
    "    obj_category_dir = os.path.join(os.path.join(data_directory,\"101_ObjectCategories\"),object_category)\n",
    "    images = [os.path.join(obj_category_dir,img) for img in os.listdir(obj_category_dir)]\n",
    "    return images\n",
    "\n",
    "def return_images_per_category(data_directory):\n",
    "    folder = os.path.join(data_directory,\"101_ObjectCategories\")\n",
    "    #print(folder)\n",
    "    categories=[d for d in os.listdir(folder) if os.path.isdir(os.path.join(folder,d))]\n",
    "    #print(categories)\n",
    "    return categories\n",
    "\n",
    "#\n",
    "# Lesen der Bilddaten aus einer Datei. Anpassen der Größe auf 300x200 (Breite x Höhe) Pixel.\n",
    "#\n",
    "def read_image(image_path):\n",
    "    #img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    #img = cv2.resize(img, (300,200), interpolation=cv2.INTER_CUBIC)    \n",
    "    im = Image.open(image_path).convert(\"RGB\").resize((300,200))\n",
    "    np_img = np.array(im)\n",
    "    return np_img\n",
    "\n",
    "\n",
    "#\n",
    "# Sammelfunktion die alle Kategorien durchgeht und die Files sammelt\n",
    "#\n",
    "def create_training_data(data_directory,fraction):\n",
    "    \n",
    "    i = 0\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    print(\"started to read dataset from {}.\".format(data_directory) )\n",
    "    \n",
    "    for category in return_images_per_category(data_directory):\n",
    "        \n",
    "        if category == 'BACKGROUND_Google':\n",
    "            continue\n",
    "        \n",
    "        print(\".\",end='')\n",
    "        \n",
    "        for image in get_images(category, data_directory):\n",
    "            if not image.endswith('.jpg'):\n",
    "                continue\n",
    "                \n",
    "            if random.uniform(0, 1) > fraction:\n",
    "                continue\n",
    "                \n",
    "            X.insert(i, read_image(image) )\n",
    "            Y.insert(i, category )\n",
    "            i += 1\n",
    "    print(\"finished reading dataset.\")\n",
    "    X = np.array(X)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Erzeugen der Trainingsdaten. Der Faktor fraction bestimmt, wieviele Daten wirklich in den Speicher geladen werden.\n",
    "# Achtung: diese Funktion kümmert sich nicht um die Gleichverteilung der Klassen.\n",
    "#\n",
    "X, Y = create_training_data(localExtractionFolder,fraction=0.4)\n",
    "\n",
    "print('data X={}, y={}'.format(X.shape, len(Y)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Transformation der Labels in one-hot encoding\n",
    "#\n",
    "label_encoder = LabelEncoder()\n",
    "Y_integer_encoded = label_encoder.fit_transform(Y)\n",
    "Y_one_hot = to_categorical(Y_integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Normalisieren der Bilddaten\n",
    "#\n",
    "X_normalized = ( X.astype(np.float64) / 255 ) + 0.001\n",
    "\n",
    "\n",
    "#\n",
    "# Löschen von X um Speicher gezielt freizumachen\n",
    "#\n",
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Split der Daten in Train und Test(validation) Datensätze\n",
    "#\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_normalized, Y_one_hot, test_size=0.25, random_state=42)\n",
    "del X_normalized\n",
    "\n",
    "\n",
    "#\n",
    "# gültige Werte in X_train, X_validation, Y_train, Y_validation, label_encoder, data_directory\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prüfen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Form der Daten\n",
    "#\n",
    "print('train: X=%s, y=%s' % (X_train.shape, Y_train.shape))\n",
    "print('test: X=%s, y=%s' % (X_validation.shape, Y_validation.shape))\n",
    "\n",
    "#\n",
    "# Plot von Bildern\n",
    "#\n",
    "for i in range(9):\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    pyplot.imshow(X_train[i])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bauen eines Modelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Erzeugen eines einfache Modelles\n",
    "#\n",
    "def createModel():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3,3), activation='relu', input_shape=(200,300,3)))\n",
    "    model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(101, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Compile und Training des Modelles\n",
    "#\n",
    "model_cnn = createModel()\n",
    "model_cnn.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Callbacks steuern das Speichern von Checkpoints und eine Überwachung gegen Overfitting.\n",
    "#\n",
    "callbacks = [ModelCheckpoint('model_cnn_weights.h5', monitor='val_acc', save_best_only=True),\n",
    "             EarlyStopping(monitor='val_loss', patience=4, verbose=1, mode='auto')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_cnn.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_data=(X_validation,Y_validation), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluierung des Modelles\n",
    "#\n",
    "_, acc = model_cnn.evaluate(X_validation, Y_validation, verbose=0)\n",
    "print('accuracy {:.3f} '.format(acc) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Ausgabe des Trainingsverlaufes\n",
    "#\n",
    "def summarize_diagnostics(history,modelname):\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_loss'], color='green', label='test')\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_accuracy'], color='green', label='test')\n",
    "    pyplot.subplots_adjust(hspace=0.5)\n",
    "    pyplot.savefig( 'results/' + modelname + '_plot.png')\n",
    "    pyplot.show()\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_diagnostics(history,'05_model_cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimiertes Laden der Bilder\n",
    "\n",
    "Die bisherige Ladefunktion hat alle Bilder in den Speicher geladen. Das führt schnell dazu, dass der Hauptspeicher ausgeht. Daher benötigen wir eine Funktion, die Bilder der Reihe nach in den Speicher lädt und für das Training zur Verfügung stellt.\n",
    "\n",
    "Eine solche Funktion kann mit einem python **Generator** implementiert werden. Die Erklärung von Generatoren ist hier zu finden [2]. Das Tutorial zum Laden mit Generatoren ist hier [1] zu finden.\n",
    "\n",
    "Quellen:\n",
    "- [1] [https://towardsdatascience.com/a-single-function-to-streamline-image-classification-with-keras-bd04f5cfe6df](https://towardsdatascience.com/a-single-function-to-streamline-image-classification-with-keras-bd04f5cfe6df)\n",
    "- [2] [https://www.python-kurs.eu/generatoren.php](https://www.python-kurs.eu/generatoren.php)\n",
    "\n",
    "\n",
    "<img src=\"info.png\" align=\"left\"/> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Anlegen eines Generators für Bilder\n",
    "#\n",
    "datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_train = datagen.flow(X_train, Y_train, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Training\n",
    "#\n",
    "model_cnn = createModel()\n",
    "model_cnn.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Neue Funktion fit_generator\n",
    "#\n",
    "steps = int(X_train.shape[0] / 64)\n",
    "history = model_cnn.fit_generator(it_train, steps_per_epoch=steps, epochs=20, validation_data=(X_validation,Y_validation), verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluierung\n",
    "#\n",
    "_, acc = model_cnn.evaluate(X_validation, Y_validation, verbose=0)\n",
    "print('accuracy {:.3f} '.format(acc) )\n",
    "summarize_diagnostics(history,'model_cnn_gen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimierung durch Augmentierung\n",
    "\n",
    "Augmentierung erweitert den Trainingsdatensatz um künstlich erzeugte Bilder. Damit wird erreicht, dass ein Modell robuster wird und sich nicht auf einzelne Pixel bezieht. Methoden der Augmentierung für Bilder sind:\n",
    "\n",
    "- Breite und Höhe des Bildinhaltes ändern (width_shift_range, height_shift_range)\n",
    "- Spiegelung (flip)\n",
    "- Rotation (rotation_range)\n",
    "- Zoomen (zoom_range)\n",
    "- Helligkeit (brightness_range)\n",
    "- Verzerrung (shear_range)\n",
    "\n",
    "Das Zufügen von Rauschen kann in Keras nicht direkt über den [ImageDataGenerator](https://keras.io/preprocessing/image/) eingestellt werden. Dies wird aber durch die Verwendung von Dropout annähernd simuliert.\n",
    "\n",
    "<img src=\"info.png\" align=\"left\"/> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Anlegen eines Generators für Bilder\n",
    "#\n",
    "datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True, rotation_range=10, zoom_range=0.1)\n",
    "# prepare iterator\n",
    "it_train = datagen.flow(X_train, Y_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Training\n",
    "#\n",
    "steps = int(X_train.shape[0] / 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = createModel()\n",
    "model_cnn.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_cnn.fit_generator(it_train, steps_per_epoch=steps, epochs=20, validation_data=(X_validation,Y_validation), verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluierung\n",
    "#\n",
    "_, acc = model_cnn.evaluate(X_validation, Y_validation, verbose=0)\n",
    "print('accuracy {:.3f} '.format(acc) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_diagnostics(history,'05_model_cnn_aug')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
