{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"header_anwender.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anwendungsbeispiel Import of sensor data and classification\n",
    "\n",
    "Ziel dieses Beispieles ist der Import und die Vorverarbeitung von Sensordaten für die Klassifizierung. Der Verwendete Datensatz ist der HAR Datensatz [1].\n",
    "\n",
    "Die Messungen der Daten wurden mit einer Gruppe von 30 Freiwilligen in einer Altersgruppe von 19-48 Jahren durchgeführt. Jede Person führte sechs Aktivitäten durch (WALKING, WALKING_UPSTAIRS, WALKING_DOWNNSTAIRS, SITTING, STANDING, LAYING) und trug ein Smartphone (Samsung Galaxy S II) auf der Hüfte. Mit Hilfe des integrierten Beschleunigungsmessers und Gyroskops haben wir die dreiachsige lineare Beschleunigung und die dreiachsige Winkelgeschwindigkeit bei einer konstanten Rate von 50 Hz erfasst. Die Experimente wurden auf Video aufgezeichnet, um die Daten manuell zu labeln. Der erhaltene Datensatz wurde nach dem Zufallsprinzip in zwei Sätze aufgeteilt, wobei 70% der Samples für die Erstellung der Trainingsdaten und 30% der Testdaten ausgewählt wurden.\n",
    "\n",
    "Die Sensorsignale (Beschleunigungsmesser und Gyroskop) wurden durch Anwendung von Rauschfiltern vorverarbeitet und dann in windows mit fester Breite von 2,56 s und 50% Überlappung abgetastet (128 Messungen/Fenster). Das Sensorbeschleunigungssignal, das eine Schwerkraft- und eine Körperbewegungskomponente aufweist, wurde mit einem Butterworth-Tiefpassfilter in Körperbeschleunigung und Schwerkraft getrennt. Es wird angenommen, dass die Gravitationskraft nur niederfrequente Komponenten aufweist, daher wurde ein Filter mit einer Grenzfrequenz von 0,3 Hz verwendet. Aus jedem Fenster wurde ein Vektor von Merkmalen durch Berechnung von Variablen aus dem Zeit- und Frequenzbereich gewonnen (Übersetzt mit www.DeepL.com/Translator).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Der Beispielcode wurde im Wesentlichen von [2] übernommen. Eine Übersicht zu anderen Methoden für die Klassifizierung von Zeitreihen ist in [3] gegeben. \n",
    "\n",
    "\n",
    "Quellen\n",
    "\n",
    "- [1] [https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones)\n",
    "- [2] [https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/](https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/)\n",
    "- [3] [https://www.mdpi.com/1424-8220/18/2/679/pdf](https://www.mdpi.com/1424-8220/18/2/679/pdf)\n",
    "- [4] [https://machinelearningmastery.com/how-to-model-human-activity-from-smartphone-data/](https://machinelearningmastery.com/how-to-model-human-activity-from-smartphone-data/)\n",
    "\n",
    "\n",
    "Zitierung für HAR Datensatz\n",
    "```\n",
    "Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from urllib.request import urlretrieve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Abdrehen von Fehlermeldungen\n",
    "#\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Für GPU Support\n",
    "#\n",
    "import tensorflow as tf\n",
    "print ( tf.__version__ ) \n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR )\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Fehler in einer Library für CPU Optimierung\n",
    "#\n",
    "# source: https://github.com/dmlc/xgboost/issues/1715\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Konstanten für Dateien\n",
    "#\n",
    "urlDataSource = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip'\n",
    "localExtractionFolder = 'data/UCIHAR'\n",
    "localDataArchive = localExtractionFolder + '/UCI HAR Dataset.zip'\n",
    "sensorData = localExtractionFolder + '/UCI HAR Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Laden der Daten von einer URL\n",
    "#\n",
    "def download_dataset(url,extraction_path,dataset_file_path):\n",
    "    if (not os.path.exists(extraction_path)):\n",
    "        os.makedirs(extraction_path)\n",
    "    if os.path.exists(localDataArchive):\n",
    "        print(\"archive already downloaded.\")\n",
    "    else:\n",
    "        print(\"started loading archive from url {}\".format(url))\n",
    "        filename, headers = urlretrieve(url, dataset_file_path)\n",
    "        print(\"finished loading archive from url {}\".format(url))\n",
    "\n",
    "def extract_dataset(dataset_file_path, extraction_directory):  \n",
    "    if (not os.path.exists(extraction_directory)):\n",
    "        os.makedirs(extraction_directory)        \n",
    "    zip = zipfile.ZipFile(dataset_file_path)\n",
    "    zip.extractall(path=extraction_directory)        \n",
    "    print(\"extraction of dataset from {} to {} done.\".format(dataset_file_path,extraction_directory) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Laden der Daten ausführen\n",
    "#\n",
    "download_dataset(urlDataSource,localExtractionFolder,localDataArchive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Extrahieren der Daten\n",
    "#\n",
    "extract_dataset(localDataArchive,localExtractionFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Laden eines Files\n",
    "#\n",
    "def load_file(filepath):\n",
    "    dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values\n",
    "\n",
    "#\n",
    "# Laden einer Gruppe (test, train)\n",
    "#\n",
    "def load_group(filenames, prefix=''):\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    loaded = np.dstack(loaded)\n",
    "    return loaded\n",
    "\n",
    "#\n",
    "# Laden aller Daten\n",
    "#\n",
    "def load_dataset ( group, prefix=''):\n",
    "    filepath = os.path.join( prefix, group, 'Inertial Signals/')\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "    # body acceleration\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "    # load input data\n",
    "    x = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + '/' + group + '/y_'+group+'.txt')\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = load_dataset('train', sensorData )\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = load_dataset('test', sensorData )\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Was sind sliding windows?\n",
    "\n",
    "Zeitreihendaten werden oft in kleinere Einheiten unterteilt. Diese Einheiten haben eine fixe Länge und werden **sliding windows** genannt. Eine gute Übersicht dazu wird in [https://datasciencetips.com/time-series-as-supervised-learning/](https://datasciencetips.com/time-series-as-supervised-learning/) gegeben.\n",
    "\n",
    "In unserem HAR Datensatz wurde das bereits im preprocessing Schritt vorbereitet. Daher brauchen wir diese Transformation nicht durchführen.\n",
    "\n",
    "<img src=\"info.png\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Überprüfen der Klassenverteilung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the balance of classes in an output variable column\n",
    "def class_breakdown(data):\n",
    "    # convert the numpy array into a dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "    # group data by the class value and calculate the number of rows\n",
    "    counts = df.groupby(0).size()\n",
    "    # retrieve raw rows\n",
    "    counts = counts.values\n",
    "    # summarize\n",
    "    for i in range(len(counts)):\n",
    "        percent = counts[i] / len(df) * 100\n",
    "        print('Class=%d, total=%d, percentage=%.3f' % (i+1, counts[i], percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize combined class breakdown\n",
    "combined = np.vstack((y_train, y_test))\n",
    "class_breakdown(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umwandlung der Klassennummer in one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train_oh = np_utils.to_categorical(y_train, 7 )\n",
    "y_test_oh = np_utils.to_categorical(y_test, 7 )\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Überprüfen eines Datensatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all data for one subject\n",
    "def data_for_subject(X, y, sub_map, sub_id):\n",
    "    ix = [i for i in range(len(sub_map)) if sub_map[i]==sub_id]\n",
    "    return X[ix, :, :], y[ix]\n",
    " \n",
    "# convert a series of windows to a 1D list\n",
    "def to_series(windows):\n",
    "    series = list()\n",
    "    for window in windows:\n",
    "        # remove the overlap from the window\n",
    "        half = int(len(window) / 2) - 1\n",
    "        for value in window[-half:]:\n",
    "            series.append(value)\n",
    "    return series\n",
    " \n",
    "# plot the data for one subject\n",
    "def plot_subject(X, y):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    # determine the total number of plots\n",
    "    n, off = X.shape[2] + 1, 0\n",
    "    # plot total acc\n",
    "    for i in range(3):\n",
    "        plt.subplot(n, 1, off+1)\n",
    "        plt.plot(to_series(X[:, :, off]))\n",
    "        plt.title('total acc '+str(i), y=0, loc='left')\n",
    "        off += 1\n",
    "    # plot body acc\n",
    "    for i in range(3):\n",
    "        plt.subplot(n, 1, off+1)\n",
    "        plt.plot(to_series(X[:, :, off]))\n",
    "        plt.title('body acc '+str(i), y=0, loc='left')\n",
    "        off += 1\n",
    "    # plot body gyro\n",
    "    for i in range(3):\n",
    "        plt.subplot(n, 1, off+1)\n",
    "        plt.plot(to_series(X[:, :, off]))\n",
    "        plt.title('body gyro '+str(i), y=0, loc='left')\n",
    "        off += 1\n",
    "    # plot activities\n",
    "    plt.subplot(n, 1, n)\n",
    "    plt.plot(y)\n",
    "    plt.title('activity', y=0, loc='left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_map = load_file( sensorData + '/train/subject_train.txt')\n",
    "train_subjects = np.unique(sub_map)\n",
    "print(train_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data for one subject\n",
    "sub_id = train_subjects[0]\n",
    "subX, suby = data_for_subject(x_train, y_train, sub_map, sub_id)\n",
    "print(subX.shape, suby.shape)\n",
    "# plot data for subject\n",
    "plot_subject(subX, suby)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bauen eines LSTM Modelles\n",
    "\n",
    "Wir verwenden eine neue Form eines Neuronalen Netzwerkes. Ein RNN ist ein Modell für die Analyse von Zeitreihen. Details dazu sind zu finden unter [https://medium.com/@purnasaigudikandula/recurrent-neural-networks-and-lstm-explained-7f51c7f6bbb9](https://medium.com/@purnasaigudikandula/recurrent-neural-networks-and-lstm-explained-7f51c7f6bbb9). \n",
    "\n",
    "<img src=\"info.png\" align=\"left\"/> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Training und Test\n",
    "#\n",
    "def evaluate_model(model,trainX, trainy, testX, testy):\n",
    "    \n",
    "    model.fit(trainX, trainy, epochs=15, batch_size=64, verbose=1)\n",
    "\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=64, verbose=0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = np.mean(scores), np.std(scores)\n",
    "    print('accuracy {:04f} (+/-{:.3f})'.format (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# run an experiment\n",
    "#\n",
    "def run_experiment(model,repeats=10):\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_model(model,x_train, y_train_oh, x_test, y_test_oh)\n",
    "        print('accuracy {:04f}'.format(score) )\n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "    return scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Erzeugen eines einfache LSTM Modelles\n",
    "#\n",
    "def createLSTMModel(trainX,trainy):\n",
    "    \n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Teste 10 Trainingsläufe und messe die Variation der Genauigkeit\n",
    "#\n",
    "model_lstm = createLSTMModel(x_train,y_train_oh)\n",
    "scores = run_experiment(model_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Anzeige der Verteilung der scores\n",
    "#\n",
    "plt.hist(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
