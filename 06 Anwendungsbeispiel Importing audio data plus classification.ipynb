{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"header_anwender.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anwendungsbeispiel Import of audio data with classification\n",
    "\n",
    "Das Ziel dieses Beispieles ist es die Arbeit mit Audiodaten, den Import, die Vorbereitung und die Klassifikation zu erklären. Dabei werden folgende Schritte durchgeführt:\n",
    "\n",
    "- Dynamisches Laden und entpacken der Audiodaten von einer externen Quelle\n",
    "- Review der Organisation auf dem Filesystem\n",
    "- Laden der Daten\n",
    "- Transformationen\n",
    "- Training\n",
    "- Analyse\n",
    "\n",
    "Der verwendete Datensatz heisst ESC-50 [1] mit 50 Klassen von Geräuschen in Dateien organisiert. Die Audiodaten sind jeweils 5 Sekunden lang und haben 40 Samples pro Klasse.\n",
    "\n",
    "\n",
    "Der Code für das Beispiel ist aus [2],[3],[4] und [5] kombiniert.\n",
    "\n",
    "\n",
    "Quellen für die Beispiele und Daten:\n",
    "\n",
    "- [1] [https://github.com/karolpiczak/ESC-50/blob/master/LICENSE](https://github.com/karolpiczak/ESC-50/blob/master/LICENSE) (Hinweise auf Unterlizenzen der Daten)\n",
    "- [2] [https://github.com/CarmineCella/esc50_keras/blob/master/esc50_keras.py](https://github.com/CarmineCella/esc50_keras/blob/master/esc50_keras.py)\n",
    "- [3] [https://medium.com/@mikesmales/sound-classification-using-deep-learning-8bc2aa1990b7](https://medium.com/@mikesmales/sound-classification-using-deep-learning-8bc2aa1990b7)\n",
    "- [4] [https://www.kaggle.com/msripooja/steps-to-convert-audio-clip-to-spectrogram](https://www.kaggle.com/msripooja/steps-to-convert-audio-clip-to-spectrogram)\n",
    "- [5] [https://ipython-books.github.io/117-creating-a-sound-synthesizer-in-the-notebook/](https://ipython-books.github.io/117-creating-a-sound-synthesizer-in-the-notebook/)\n",
    "\n",
    "Zitat der Datenquelle:\n",
    "```\n",
    "K. J. Piczak. ESC: Dataset for Environmental Sound Classification. Proceedings of the 23rd Annual ACM Conference on Multimedia, Brisbane, Australia, 2015.\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import joblib\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import os.path\n",
    "import zipfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import History\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from sklearn.svm import SVC\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#\n",
    "# Abdrehen von Fehlermeldungen\n",
    "#\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=Warning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Für GPU Support\n",
    "#\n",
    "import tensorflow as tf\n",
    "print ( tf.__version__ ) \n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR )\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlDataSource = 'https://github.com/karoldvl/ESC-50/archive/master.zip'\n",
    "localExtractionFolder = 'data/ESC-50'\n",
    "localDataArchive = 'data/ESC-50/master.zip'\n",
    "audioData = localExtractionFolder + '/ESC-50-master/audio'\n",
    "\n",
    "sampleRate = 22050\n",
    "sampleLen = 110250 # in samples is 5 sec @ 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Laden der Daten von einer URL\n",
    "#\n",
    "def download_dataset(url,extraction_path,dataset_file_path):\n",
    "    if (not os.path.exists(extraction_path)):\n",
    "        os.makedirs(extraction_path)\n",
    "    if os.path.exists(localDataArchive):\n",
    "        print(\"archive already downloaded.\")\n",
    "    else:\n",
    "        print(\"started loading archive from url {}\".format(url))\n",
    "        filename, headers = urlretrieve(url, dataset_file_path)\n",
    "        print(\"finished loading archive from url {}\".format(url))\n",
    "\n",
    "def extract_dataset(dataset_file_path, extraction_directory):    \n",
    "    if (not os.path.exists(extraction_directory)):\n",
    "        os.makedirs(extraction_directory)        \n",
    "    zip = zipfile.ZipFile(dataset_file_path)\n",
    "    zip.extractall(path=extraction_directory)        \n",
    "    print(\"extraction of dataset from {} to {} done.\".format(dataset_file_path,extraction_directory) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laden der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Laden der Daten ausführen\n",
    "#\n",
    "download_dataset(urlDataSource,localDataArchive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Extrahieren der Daten\n",
    "#\n",
    "extract_dataset(localDataArchive,localExtractionFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organisation von Audiodaten auf dem Filesystem\n",
    "\n",
    "Die Audiodateien liegen alle in einem Verzeichnis. Die Zuordnung der Klasse ist im Dateinamen kodiert und in einer Datenbank gespeichert.\n",
    "Details dazu unter [[1]](https://github.com/karolpiczak/ESC-50)\n",
    "\n",
    "<img src=\"info.png\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Auslesen der Datenbank\n",
    "#\n",
    "df = pd.read_csv( localExtractionFolder + '/ESC-50-master/meta/esc50.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Sammeln der Klasseninformation\n",
    "#\n",
    "classes = df[['target', 'category']].values.tolist()\n",
    "classes = set(['{} {}'.format(c[0], c[1]) for c in classes])\n",
    "classes = np.array([c.split(' ') for c in classes])\n",
    "classes = {k: v for k, v in classes}\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Einlesen von 5 files\n",
    "#\n",
    "\n",
    "x_check = []\n",
    "count = 0\n",
    "for root, dir, files in os.walk(audioData):\n",
    "    waves = fnmatch.filter(files, \"*.wav\")\n",
    "    for item in waves:\n",
    "        soundFile = os.path.join(root, item)\n",
    "        yt, sr = librosa.core.load (soundFile, mono=True)\n",
    "\n",
    "        print('found file {} with data shape {} and sampling rate {}'.format(soundFile,yt.shape,sr))\n",
    "        \n",
    "        x_check.append(yt)\n",
    "        count = count + 1\n",
    "        if count > 5:\n",
    "            break\n",
    "\n",
    "x_check = np.array(x_check)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Anzeige als PCM Kurven\n",
    "#\n",
    "displayIndex = 1\n",
    "x_show = x_check[displayIndex]\n",
    "\n",
    "plt.figure(figsize=(11, 5))\n",
    "librosa.display.waveplot(x_show, sr=sampleRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = librosa.stft(x_show)\n",
    "Xdb = librosa.amplitude_to_db(abs(X))\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n",
    "plt.colorbar()\n",
    "plt.figure(figsize=(11.4, 5))\n",
    "librosa.display.waveplot(x_show, sr=sampleRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import (\n",
    "    Audio, display, clear_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(x_show, rate=sampleRate, autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erzeugen der Trainingsdaten\n",
    "\n",
    "https://en.wikipedia.org/wiki/Constant-Q_transform\n",
    "\n",
    "<img src=\"info.png\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Berechnen der Featuretransformation für Audio\n",
    "# cqt \n",
    "# \n",
    "\n",
    "# step size\n",
    "window = 1024\n",
    "# frequency pins\n",
    "bins = 64\n",
    "\n",
    "def get_features (file, hop, bins):\n",
    "    \n",
    "    y = np.zeros(sampleLen);   \n",
    "    yt, sr = librosa.core.load (file, mono=True)\n",
    "    \n",
    "    if len(yt) == 0: \n",
    "        print ('found empty file ' + file )\n",
    "        return 0\n",
    "\n",
    "    min_length = min(len(y), len(yt))\n",
    "    y[:min_length] = yt[:min_length]\n",
    "    \n",
    "    # https://librosa.github.io/librosa/generated/librosa.feature.mfcc.html\n",
    "    #C = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop, n_mfcc = bins)  \n",
    "\n",
    "    #https://librosa.github.io/librosa/generated/librosa.core.cqt.html\n",
    "    C = np.log1p( 1000 * np.abs (librosa.core.cqt( y=y, sr=sr, hop_length=hop, n_bins=bins)))\n",
    "    \n",
    "    return C\n",
    "\n",
    "memory = joblib.Memory(cachedir=localExtractionFolder+'/esc50_joblib', verbose=0)\n",
    "cached_get_features = memory.cache(get_features)\n",
    "\n",
    "\n",
    "def compute_features (root_path):\n",
    "        \n",
    "    classes = 50\n",
    "    samples = 0\n",
    "\n",
    "    y_data = []    \n",
    "    X_data = []\n",
    "    \n",
    "    for root, dir, files in os.walk(root_path):\n",
    "        \n",
    "        waves = fnmatch.filter(files, \"*.wav\")\n",
    "\n",
    "        if len(waves) != 0:\n",
    "            for item in waves:\n",
    "                # e.g. 2-39443-A-19.wav\n",
    "                fileName = os.path.splitext ( os.path.basename(item) )[0] \n",
    "                classID = int(fileName.split('-')[3])\n",
    "                \n",
    "                mfcc = cached_get_features( os.path.join(root, item), window, bins)\n",
    "                print(\".\",end='')\n",
    "\n",
    "                X_data.append(mfcc)\n",
    "                y_data.append(classID)\n",
    "                \n",
    "                samples = samples + 1\n",
    "                if samples >= 100:\n",
    "                    break\n",
    "\n",
    "    X_data = np.stack(X_data, axis=2)\n",
    "    \n",
    "    print('shape features {}'.format(X_data.shape))\n",
    "    \n",
    "    X_data = np.transpose(X_data, (2,0,1))\n",
    "    d1 = X_data.shape[0]\n",
    "    d2 = X_data.shape[1]\n",
    "    d3 = X_data.shape[2]    \n",
    "    X_data = np.reshape(X_data, (d1,d2,d3,1))\n",
    "    y_data = np.array(y_data)    \n",
    "\n",
    "    print('shape transformed {}'.format(X_data.shape))\n",
    "\n",
    "    print (\"samples = \" + str (samples))\n",
    "\n",
    "    return X_data, y_data, classes, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data, classes, samples = compute_features (audioData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data.astype('float32')\n",
    "y_data = y_data.astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anzeige der Features als Bild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_show = x_data[displayIndex]\n",
    "x_show = np.reshape(x_show, (64,108))\n",
    "#x_show = np.transpose(x_show,(1,0))\n",
    "plt.imshow(x_show)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisieren der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize (x):\n",
    "    mu = np.mean (x, axis=0)\n",
    "    de = np.std (x, axis=0)\n",
    "    \n",
    "    eps = np.finfo('float32').eps\n",
    "    x = (x - mu) / (eps + de)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = standardize(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_data = np_utils.to_categorical(y_data, classes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Nochmals checken\n",
    "#\n",
    "x_show = x_data[displayIndex]\n",
    "x_show = np.reshape(x_show, (64,108))\n",
    "#x_show = np.transpose(x_show,(1,0))\n",
    "plt.imshow(x_show)\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train und Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Split der Daten in Train und Test(validation) Datensätze\n",
    "#\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_data, y_data, test_size=0.25, random_state=42)\n",
    "\n",
    "print('shapes {} {} {} {}'.format(x_train.shape, x_validation.shape, y_train.shape, y_validation.shape ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model bauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Erzeugen eines einfache Modelles\n",
    "#\n",
    "def createModel():\n",
    "    \n",
    "    img_rows = x_train.shape[1]\n",
    "    img_cols = x_train.shape[2]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, kernel_size=5, strides=1, border_mode='same', input_shape=(img_rows, img_cols, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(32, kernel_size=3, strides=1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.6))\n",
    "     \n",
    "    model.add(Convolution2D(64, kernel_size=3, strides=1, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, kernel_size=3, strides=1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Anlegen des Modelles mit Beschreibung\n",
    "#\n",
    "model_cnn = createModel()\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Festlegen des Optimizers\n",
    "#\n",
    "optimizer = Adam ()\n",
    "model_cnn.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Anlegen des Datengenerators mit Augmentierung\n",
    "#\n",
    "datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.4,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.4,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=False,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Training des Generators\n",
    "#\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [History()]\n",
    "#callbacks = [EarlyStopping(monitor='val_loss', patience=4, verbose=1, mode='auto'), History()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Training des Modelles\n",
    "#\n",
    "history = model_cnn.fit_generator(datagen.flow(x_train, y_train,\n",
    "                            batch_size=64),\n",
    "                            samples_per_epoch=x_train.shape[0],\n",
    "                            nb_epoch=2400,\n",
    "                            callbacks=callbacks,\n",
    "                            validation_data=(x_validation, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Prüfung des Modelles\n",
    "#\n",
    "score = model_cnn.evaluate(x_validation, y_validation, verbose=0)\n",
    "print(\"validation {} {:.3f}\" .format(model_cnn.metrics_names[1], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Ausgabe des Trainingsverlaufes\n",
    "#\n",
    "def summarize_diagnostics(history,modelname):\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_loss'], color='green', label='test')\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_accuracy'], color='green', label='test')\n",
    "    pyplot.subplots_adjust(hspace=0.5)\n",
    "    pyplot.savefig( 'results/' + modelname + '_plot.png')\n",
    "    pyplot.show()\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_diagnostics(history,'06_model_cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Speichern des Modelles\n",
    "#\n",
    "from keras.models import model_from_json\n",
    "prefix = 'results/06_'\n",
    "modelName = prefix + \"model.json\"\n",
    "weightName = prefix + \"model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model_json = model_cnn.to_json()\n",
    "    with open( modelName , \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model_cnn.save_weights( weightName )\n",
    "    print(\"saved model to disk as {} {}\".format(modelName,weightName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Laden eines vortrainierten Modelles\n",
    "#\n",
    "if False:\n",
    "    json_file = open(modelName, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(weightName)\n",
    "    print(\"loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# evaluate loaded model on test data\n",
    "#\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(x_validation, y_validation, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
