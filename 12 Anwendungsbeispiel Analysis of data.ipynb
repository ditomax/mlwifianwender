{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"header.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anwendungsbeispiel Analysis and quality control of data \n",
    "\n",
    "Die Ziel dieses Beispieles ist es die Methoden der Datenanalyse nochmals zu zeigen und deren Notwendigkeit zu begründen.\n",
    "\n",
    "\n",
    "- Datentypen und Form der Daten\n",
    "- Visualisierung\n",
    "- Fehlende Daten\n",
    "- Statistische Werte\n",
    "- Outliers und Anomalien in den Daten\n",
    "- Korrelationen und Beziehungen zwischen den Features\n",
    "- Untersuchung der Trainingsdaten\n",
    "- Stabilere Prüfung der Modellqualität\n",
    "\n",
    "\n",
    "Code und Informationen entnommen von:\n",
    "\n",
    "- [https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba)\n",
    "- [https://towardsdatascience.com/5-ways-to-detect-outliers-that-every-data-scientist-should-know-python-code-70a54335a623](https://towardsdatascience.com/5-ways-to-detect-outliers-that-every-data-scientist-should-know-python-code-70a54335a623)\n",
    "- [https://github.com/Viveckh/HiPlotTutorial/blob/master/Hiplot-Tutorial.ipynb](https://github.com/Viveckh/HiPlotTutorial/blob/master/Hiplot-Tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import der Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Import der Module\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hiplot as hip\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Abdrehen von Fehlermeldungen\n",
    "#\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "#\n",
    "# Einstellen der Grösse von Diagrammen\n",
    "#\n",
    "plt.rcParams['figure.figsize'] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datentypen und Form der Daten\n",
    "\n",
    "https://numpy.org/devdocs/user/basics.types.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Laden der Daten\n",
    "# \n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "iris = pd.read_csv('data/iris/iris_mutilated.csv', names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Anzeige der Form der Daten\n",
    "#\n",
    "print(iris.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Anzeige der Datentypen\n",
    "#\n",
    "print(iris.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Anzeige von Samples zur Visualisierung der Inhalte\n",
    "#\n",
    "print(iris.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fehlende Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Liste alle Reihen mit fehlenden Werten\n",
    "# any(axis=1) liefert ein True, wenn eines der Features über die axis 1 True ist\n",
    "# \n",
    "iris[iris.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Ersetzen durch Mittelwert des Features (grauslich)\n",
    "#\n",
    "iris_non = iris.fillna(iris.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_non[iris_non.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Füllen von fehlenden Werte mit dem Mittelwert des Features erzeugt neue Datenpunkte, die potentiell störend sind. Alternativ können die Datenpunkte gelöscht werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplikate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Prüfung auf Duplikate zeigt zwar doppelte Werte, aber keine massiven Störungen\n",
    "# bis auf Weiteres keine Änderung \n",
    "#\n",
    "iris_non[iris_non.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einfache Statistiken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Verteilung der Labels (Klassen)\n",
    "#\n",
    "print(iris_non.groupby('class').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Histogramm der Klassen als Plot \n",
    "#\n",
    "#\n",
    "# Prüfen der Verteilung der Klassen\n",
    "#\n",
    "df = pd.DataFrame(iris_non,columns=['class'])\n",
    "counts= df.groupby('class').size()\n",
    "class_pos = np.arange(3)\n",
    "plt.bar(class_pos, counts, align='center', alpha=0.4)\n",
    "plt.xlabel(class_pos)\n",
    "plt.ylabel('Ziffern')\n",
    "plt.title('Samples pro Ziffer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Analyse der Verteilung der Werte in den Features als Tabelle\n",
    "#\n",
    "iris_non.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers und Anomalien in den Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Analyse der Verteilung der Werte in den Features als Boxplot (outliers)\n",
    "#\n",
    "iris_non.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Mathematische Berechnung\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = iris_non.values[:,:-1].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.abs(stats.zscore(values))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Filtern aller Werte mit z-score >= 3\n",
    "#\n",
    "iris_non_noo = iris_non[(z < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_non_noo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Analyse der Verteilung der Werte in den Features als Boxplot (outliers)\n",
    "#\n",
    "iris_non_noo.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Durch Clustering (Teil des KI Profi Kurses)\n",
    "#\n",
    "outlier_detection = DBSCAN( min_samples = 2, eps = 0.6 )\n",
    "clusters = outlier_detection.fit_predict( iris_non_noo.values[:,:-1] )\n",
    "list(clusters).count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Zu welchen clustern wurden die Samples zugeordnet? Outliers werden mit -1 markiert.\n",
    "#\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korrelationen und Beziehungen zwischen den Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Analyse der Verteilung der Werte in den Features als Histogram\n",
    "#\n",
    "iris_non_noo.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Mathematische Analyse der Beziehungen zwischen den Features (Korrelation)\n",
    "#\n",
    "iris_non.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(iris_non_noo.corr(),annot=True,cmap='Blues_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Darstellung der Beziehungen zwischen den Features als Matrix Plot\n",
    "#\n",
    "scatter_matrix(iris_non_noo)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Darstellung der Beziehungen zwischen den Features als Matrix Plot mit Unterscheidung der Klassen\n",
    "# Eine wichtige Fragestellung ist dabei die Separierbarkeit\n",
    "#\n",
    "sns.pairplot(iris_non_noo,hue='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Neue Form der Darstellung der Zusammenhänge zwischen Features und Klassen\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = iris_non_noo.to_dict('records')\n",
    "iris_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hip.Experiment.from_iterable(iris_data).display(force_full_width=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prüfung der Modellqualität und implizit auch der Trainingsdatenqualität\n",
    "\n",
    "[https://scikit-learn.org/stable/modules/cross_validation.html](https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Aufteilen in Training Daten und Testdaten\n",
    "#\n",
    "array = iris_non_noo.values\n",
    "X = array[:,0:4]\n",
    "Y = array[:,4]\n",
    "validation_size = 0.01\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Test einer Reihe von Modellen gleichzeitig. Dabei werden mehrere Verteilungen von Trainingsdaten\n",
    "# erzeugt und trainiert. Dadurch wird sichtbar, wenn die Verteilung der Trainingsdaten nicht ausreichend\n",
    "# breit ist (hohe Varianz). Mehr zu weiteren Methoden unter:\n",
    "# https://www.pluralsight.com/guides/validating-machine-learning-models-scikit-learn\n",
    "#\n",
    "\n",
    "scoring = 'accuracy'\n",
    "\n",
    "# Modelle anlegen\n",
    "models = []\n",
    "models.append(('KNN', KNeighborsClassifier(n_neighbors=5,metric='euclidean')))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Testen mit mehreren unterschiedlichen zufälligen Aufteilungen der Daten\n",
    "#\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    \n",
    "    kfold = model_selection.KFold( n_splits=10, random_state=42,shuffle=True)\n",
    "    \n",
    "    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    print(\"Modell {}: accuracy {:.3f} (deviation {:.3f})\".format(name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Testen mit mehreren unterschiedlichen Aufteilungen der Daten wobei die Klassenverteilung gleich bleibt\n",
    "#\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    \n",
    "    skfold = model_selection.StratifiedKFold(n_splits=10, random_state=42,shuffle=True)\n",
    "    \n",
    "    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=skfold, scoring=scoring)\n",
    "    print(\"Modell {}: accuracy {:.3f} (deviation {:.3f})\".format(name, cv_results.mean(), cv_results.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Untersuchung der Trainingsdaten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Laden der MNIST Daten und des Modelles\n",
    "#\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# MNIST Daten mit Transformationen\n",
    "(x_train, y_train), (_, _) = mnist.load_data()\n",
    "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
    "x_train = x_train.astype('float32')\n",
    "x_train = x_train / 255.0\n",
    "y_train = to_categorical(y_train, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Anzeige der Anzahl und Form der Samples\n",
    "#\n",
    "print('Trainingsdaten: X=%s, y=%s' % (x_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell aus Beispiel 4\n",
    "from keras.models import model_from_json\n",
    "prefix = 'results/04_'\n",
    "modelName = prefix + \"model.json\"\n",
    "weightName = prefix + \"model.h5\"\n",
    "json_file = open(modelName, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "# model aus json\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# gewichte aus h5 file\n",
    "loaded_model.load_weights(weightName)\n",
    "print(\"loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = x_train[0].reshape((1,28,28,1))    \n",
    "prediction_activation = loaded_model.predict([image])\n",
    "predictedClass = np.argmax ( prediction_activation[0] )\n",
    "confidence = prediction_activation[0][predictedClass]\n",
    "predictedClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Untersuchung aller Trainingsdaten mit Modell und Suche nach niedriger confidence\n",
    "#\n",
    "\n",
    "predictionConfidencePerClass = [[] for i in range(10)]\n",
    "errorCount = 0\n",
    "errorCountDistribution = [0] * 10\n",
    "suspectList = []\n",
    "suspectListConfidence = []\n",
    "\n",
    "for i in range( x_train.shape[0] ):\n",
    "    \n",
    "    correctClass = np.argmax(y_train[i])\n",
    "    image = x_train[i].reshape((1,28,28,1))\n",
    "    prediction_activation = loaded_model.predict([image])\n",
    "    predictedClass = np.argmax ( prediction_activation[0] )\n",
    "    confidence = prediction_activation[0][predictedClass]\n",
    "    if predictedClass != correctClass:\n",
    "        errorCountDistribution[correctClass] = errorCountDistribution[correctClass] + 1\n",
    "    else:\n",
    "        if confidence < 0.9:\n",
    "            predictionConfidencePerClass[correctClass].append(confidence)\n",
    "            errorCount+= 1\n",
    "            \n",
    "        if confidence < 0.6:\n",
    "            suspectList.append(i)\n",
    "            suspectListConfidence.append(confidence)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Anzahl der gefundenen Fälle ist {}'.format(errorCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionConfidencePerClassNP = np.asarray(predictionConfidencePerClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clazzz in range(10):\n",
    "    # Subset to the airline\n",
    "    \n",
    "    subset = predictionConfidencePerClassNP[clazzz]\n",
    "    \n",
    "    # Draw the density plot\n",
    "    sns.distplot(subset, hist = False, kde = True, kde_kws = {'linewidth': 3}, label = 'Ziffer {}'.format(clazzz) )\n",
    "    \n",
    "# Plot formatting\n",
    "plt.legend(prop={'size': 10}, title = 'digits')\n",
    "plt.title('Confidence of classifications below 0.9')\n",
    "plt.xlabel('confidence')\n",
    "plt.ylabel('density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "fig.suptitle('Suspect cases')\n",
    "plotCount = 0\n",
    "\n",
    "for i in range(len(suspectList)):\n",
    "    correctClass = np.argmax(y_train[i])\n",
    "    if plotCount < 9 and correctClass == 3:\n",
    "            ax = plt.subplot(330 + 1 + plotCount)\n",
    "            ax.set_title('susp {} conf {:.2f} digit {}'.format ( i, suspectListConfidence[i], correctClass ) )     \n",
    "            image = x_train[i].reshape((28,28))\n",
    "            plt.imshow(image, cmap=plt.get_cmap('gray'))    \n",
    "            plotCount+= 1\n",
    "        \n",
    "\n",
    "plt.subplots_adjust(wspace=0.4,hspace=0.4)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
